---
layout: post
categories: notes
title: 'Notes: Community knowledge assessment in a knowledge building environment'
tags:
- knowledge building
- assessment
---

## References

**Citekey**: @Hong2014

Hong, H.-Y. and Scardamalia, M. (2014). Community knowledge assessment in a knowledge building environment. Computers & Education, 71:279–288.


## Notes

## Highlights


According to Stahl (2006) group cognition requires a theory of collaboration that takes the group as the unit of analysis rather than the individual. ‘Learning by groups’ is not the same as ‘learning in groups’ or individual learning through social processes. (p. 1)

1.1. Community knowledge (p. 1)

To understand the nature of group or community knowledge, it is important to distinguish between a psychological concept of knowledge as something within an individual mind and a social concept of knowledge as conceptual artifacts that have a public life (Bereiter, 2002; Bereiter & Scardamalia, 1996; Hyman, 1999; Popper, 1972). Community knowledge is public knowledged ideas made accessible to all community members through contributions to collective knowledge spaces. Thus ideas have a life beyond the individual mind and can be continually accessed and improveddmuch as is the case with publication in journals. Community knowledge involves a dynamic processd interactions between ideas and people knowledge (i.e., knowing people’s expertise)dwith participants monitoring who is working on what ideas or problems and advancing knowledge in the community (Hong & Lin-Siegler, 2012). (p. 1)

1.2. Assessing community knowledge (p. 2)

Despite its time-consuming nature, a common way to assess group or community knowledge is to assess group products (Bourner, Hughes, & Bourner, 2001; Cheng & Warren, 2000; Lee, Chan, & van Aalst, 2006; Lejk & Wyvill, 2001). (p. 2)

A commonly used unit of content analysis has been theme. Themes can be either pre-speci"ed based on an existing protocol or emergent from an open-coding process (Strauss & Corbin, 1990). But either way, there is an issue of subjectivity of theme selection. Second, content analysis has been used more often by researchers as a research tool than by users as an interaction/learning tool for assessing content in a database. Particularly for teachers (and students as well), it is a dif"cult and time-consuming technique to apply, thus limiting its use and usefulness for advancing the state of community knowledge. (p. 2)

Therefore, the purpose of this study is to explore possibilities of using key-term measures: (1) to help assess and visually represent both personal and community knowledge, and (2) to complement (not replace) traditional online behavioral measures, in order to better capture knowledge building dynamics. (p. 3)

2. Method (p. 3)

2.4. Proposed key-term measures (p. 4)

Table 1 summarizes all key measures employed in this study and the statistics used to compare between the two datasets by using the conventional and new measures. (p. 5)

3.1. Comparisons using conventional online behavioral measures (p. 5)

In other words, in the second phase there was (1) less time spent on problem generation and connectivity (i.e., note-linking and building-on), and (2) more time on elaborating the content of each note (e.g., more note revisions and more words per note). This suggests a change from more breadth-oriented inquiry in phase 1 to more re!ective, depthoriented inquiry in phase 2. This con"rms "ndings from the previous study mentioned above (Hong et al., 2008). (p. 6)

It is worth noting that the standard deviations seemed rather high (e.g., the number of note revisions). This may be because knowledge building highlights student autonomy, whereas traditional didactic teaching tends to maintain a relatively equal learning agenda and pace for a group of students. Therefore, it is possible that variations in performance re!ect greater diversity in the activities engaged in by knowledge builders. (p. 6)

3.2. Comparisons using alternative key-term measures (p. 6)

First, in terms of unique key terms when comparing the mean number of key terms each student contributed between the two phases, it was found that there was no signi"cant difference (t 1⁄4 2.065, df 1⁄4 21, P > .05). On average, each student generated 40.10 key terms (SD 1⁄4 17.74) in phase 1 and 33.27 key terms (SD 1⁄4 11.35) in phase 2 (see Table 3). However, when comparing the mean frequency of key-term use by each student between the two phases, there was signi"cant difference (t 1⁄4 2.483, df 1⁄4 21, P < .05) in that the mean frequency of key-term use is 78.41 (SD 1⁄4 38.74) in phase 1 and 100.14 (SD 1⁄4 40.11) in phase 2. (p. 6)

Overall, similarities between the conventional and key-term measures showed the following important patterns: First, both types of measures can be used for automated assessment. Second, both can be easily integrated into online learning environments. Third, both are easy to implement, and with careful instructiona (p. 8)

Previous studies suggest that after continuously solving many problems in their area of expertise, expert learners (or teams of expert learners) possess a stronger sense of which ideas are more promising to pursue in "nding a solution to a problem, and/or of how to improve, re"ne, or redesign the problem space (e.g., Schwartz & Martin, 2004). (p. 9)
